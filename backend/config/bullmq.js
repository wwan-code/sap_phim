import { Queue, Worker, QueueEvents } from 'bullmq';
import IORedis from 'ioredis';
import dotenv from 'dotenv';
import logger from '../utils/logger.js';

dotenv.config();

// ==================== REDIS CONNECTION POOL ====================
// Táº¡o connection pool Ä‘á»ƒ tÃ¡i sá»­ dá»¥ng káº¿t ná»‘i, giáº£m overhead
const createRedisConnection = () => new IORedis({
  host: process.env.REDIS_HOST,
  port: parseInt(process.env.REDIS_PORT || '6379', 10),
  password: process.env.REDIS_PASSWORD,
  maxRetriesPerRequest: null,
  enableOfflineQueue: true,
  retryStrategy: (times) => {
    const delay = Math.min(times * 50, 2000);
    logger.warn(`Redis retry attempt ${times}, waiting ${delay}ms`);
    return delay;
  },
  reconnectOnError: (err) => {
    const targetError = 'READONLY';
    if (err.message.includes(targetError)) {
      logger.error('Redis READONLY error, reconnecting...');
      return true;
    }
    return false;
  },
  // Connection pool settings
  lazyConnect: false,
  keepAlive: 30000,
  connectTimeout: 10000,
  // Performance tuning
  enableReadyCheck: true,
  maxLoadingRetryTime: 10000,
});

// Táº¡o cÃ¡c connection riÃªng biá»‡t cho Queue, Worker, Events Ä‘á»ƒ trÃ¡nh conflict
const connectionQueue = createRedisConnection();
const connectionWorker = createRedisConnection();
const connectionEvents = createRedisConnection();

// Event handlers cho monitoring
[connectionQueue, connectionWorker, connectionEvents].forEach((conn, idx) => {
  const names = ['Queue', 'Worker', 'Events'];
  conn.on('error', (err) => logger.error(`Redis ${names[idx]} error:`, err));
  conn.on('connect', () => logger.info(`Redis ${names[idx]} connected`));
  conn.on('ready', () => logger.info(`Redis ${names[idx]} ready`));
  conn.on('close', () => logger.warn(`Redis ${names[idx]} closed`));
  conn.on('reconnecting', () => logger.warn(`Redis ${names[idx]} reconnecting...`));
});

// ==================== QUEUE CONFIGURATION ====================
/**
 * Cáº¥u hÃ¬nh Queue vá»›i cÃ¡c options tá»‘i Æ°u cho video processing
 * - Sá»­ dá»¥ng priority Ä‘á»ƒ xá»­ lÃ½ cÃ¡c video quan trá»ng trÆ°á»›c
 * - Rate limiting Ä‘á»ƒ trÃ¡nh overload server
 * - Retry strategy thÃ´ng minh
 * 
 * NOTE: Tá»« BullMQ v5, QueueScheduler Ä‘Ã£ Ä‘Æ°á»£c loáº¡i bá».
 * Stalled job handling giá» Ä‘Æ°á»£c xá»­ lÃ½ tá»± Ä‘á»™ng bá»Ÿi Worker.
 */
export const reelQueue = new Queue('reelProcessingQueue', {
  connection: connectionQueue,
  defaultJobOptions: {
    // Tá»± Ä‘á»™ng xÃ³a job sau khi hoÃ n thÃ nh Ä‘á»ƒ tiáº¿t kiá»‡m memory
    removeOnComplete: {
      age: 24 * 3600, // Giá»¯ job 24h sau khi hoÃ n thÃ nh
      count: 1000, // Giá»¯ tá»‘i Ä‘a 1000 jobs
    },
    removeOnFail: {
      age: 7 * 24 * 3600, // Giá»¯ job lá»—i 7 ngÃ y Ä‘á»ƒ debug
    },
    attempts: 3, // Retry tá»‘i Ä‘a 3 láº§n
    backoff: {
      type: 'exponential',
      delay: 5000, // 5s, 10s, 20s
    },
    // Priority cao hÆ¡n = xá»­ lÃ½ trÆ°á»›c
    priority: 1,
  },
});

logger.info('âœ… Reel Queue initialized');

// ==================== WORKER SETUP ====================
/**
 * Setup Worker vá»›i concurrency vÃ  rate limiting
 * @param {Function|string} processorPath - Processor function hoáº·c path Ä‘áº¿n file
 * @param {Object} options - Worker options
 */
export const setupReelWorker = (processorPath, options = {}) => {
  const defaultOptions = {
    connection: connectionWorker,
    // Concurrency: sá»‘ job xá»­ lÃ½ Ä‘á»“ng thá»i
    // CÃ¢n nháº¯c CPU/Memory: 3-5 cho server vá»«a, 10+ cho server máº¡nh
    concurrency: parseInt(process.env.WORKER_CONCURRENCY || '3', 10),
    
    // Rate limiter: giá»›i háº¡n sá»‘ job/giÃ¢y Ä‘á»ƒ trÃ¡nh overload
    limiter: {
      max: parseInt(process.env.WORKER_MAX_JOBS_PER_SECOND || '5', 10),
      duration: 1000,
      // Grouping Ä‘á»ƒ limit theo user/group náº¿u cáº§n
      groupKey: 'userId', // Rate limit per user
    },
    
    // Lock settings
    lockDuration: 300000, // 5 phÃºt
    lockRenewTime: 15000, // Renew lock má»—i 15s
    
    // Performance tuning
    runRetryDelay: 5000, // Äá»£i 5s trÆ°á»›c khi retry
    
    // Advanced settings
    settings: {
      // Backoff strategy khi khÃ´ng cÃ³ job
      backoffStrategies: {
        exponential: (attemptsMade) => Math.min(attemptsMade * 1000, 30000),
      },
    },
  };

  const worker = new Worker(
    'reelProcessingQueue',
    processorPath,
    { ...defaultOptions, ...options }
  );

  // ==================== WORKER EVENT HANDLERS ====================
  
  worker.on('completed', (job) => {
    logger.info(`âœ… Job ${job.id} completed for reel ${job.data.reelId}`, {
      duration: Date.now() - job.processedOn,
      attempts: job.attemptsMade,
    });
  });

  worker.on('failed', (job, err) => {
    logger.error(`âŒ Job ${job?.id} failed for reel ${job?.data?.reelId}`, {
      error: err.message,
      stack: err.stack,
      attempts: job?.attemptsMade,
      data: job?.data,
    });
  });

  worker.on('active', (job) => {
    logger.info(`ðŸ”„ Job ${job.id} started for reel ${job.data.reelId}`, {
      attemptsMade: job.attemptsMade,
      timestamp: new Date().toISOString(),
    });
  });

  worker.on('progress', (job, progress) => {
    logger.debug(`ðŸ“Š Job ${job.id} progress: ${progress}%`);
  });

  worker.on('stalled', (jobId) => {
    logger.warn(`âš ï¸ Job ${jobId} has stalled and will be reprocessed`);
  });

  worker.on('error', (err) => {
    logger.error('Worker error:', err);
  });

  // ==================== QUEUE EVENTS ====================
  /**
   * QueueEvents láº¯ng nghe cÃ¡c event cá»§a queue tá»« Redis
   * PhÃ¹ há»£p cho monitoring vÃ  logging
   */
  const queueEvents = new QueueEvents('reelProcessingQueue', {
    connection: connectionEvents,
  });

  queueEvents.on('completed', ({ jobId, returnvalue }) => {
    logger.debug(`âœ… QueueEvents: Job ${jobId} completed`, {
      returnvalue: returnvalue?.substring?.(0, 100), // Log first 100 chars
    });
  });

  queueEvents.on('failed', ({ jobId, failedReason }) => {
    logger.error(`âŒ QueueEvents: Job ${jobId} failed: ${failedReason}`);
  });

  queueEvents.on('progress', ({ jobId, data }) => {
    logger.debug(`ðŸ“Š QueueEvents: Job ${jobId} progress:`, data);
  });

  queueEvents.on('stalled', ({ jobId }) => {
    logger.warn(`âš ï¸ QueueEvents: Job ${jobId} stalled`);
  });

  queueEvents.on('waiting', ({ jobId }) => {
    logger.debug(`â³ QueueEvents: Job ${jobId} is waiting`);
  });

  // Graceful shutdown
  const cleanup = async () => {
    logger.info('ðŸ›‘ Shutting down Reel Worker...');
    await worker.close();
    await queueEvents.close();
    logger.info('âœ… Reel Worker closed gracefully');
  };

  process.on('SIGTERM', cleanup);
  process.on('SIGINT', cleanup);

  return { worker, queueEvents };
};

// ==================== QUEUE UTILITIES ====================

/**
 * Add job vá»›i priority vÃ  delay
 * @param {Object} jobData - Job data
 * @param {Object} options - Job options (priority, delay, etc.)
 */
export const addReelProcessingJob = async (jobData, options = {}) => {
  try {
    const job = await reelQueue.add('processReel', jobData, {
      priority: options.priority || 1, // Cao hÆ¡n = quan trá»ng hÆ¡n
      delay: options.delay || 0, // ms delay trÆ°á»›c khi process
      jobId: options.jobId, // Custom job ID náº¿u cáº§n
      ...options,
    });

    logger.info(`âž• Added job ${job.id} to queue`, {
      reelId: jobData.reelId,
      priority: job.opts.priority,
    });

    return job;
  } catch (error) {
    logger.error('Failed to add job to queue:', error);
    throw error;
  }
};

/**
 * Get queue metrics cho monitoring
 */
export const getQueueMetrics = async () => {
  try {
    const [waiting, active, completed, failed, delayed] = await Promise.all([
      reelQueue.getWaitingCount(),
      reelQueue.getActiveCount(),
      reelQueue.getCompletedCount(),
      reelQueue.getFailedCount(),
      reelQueue.getDelayedCount(),
    ]);

    return {
      waiting,
      active,
      completed,
      failed,
      delayed,
      total: waiting + active + completed + failed + delayed,
      timestamp: new Date().toISOString(),
    };
  } catch (error) {
    logger.error('Failed to get queue metrics:', error);
    return null;
  }
};

/**
 * Clean old jobs Ä‘á»ƒ trÃ¡nh memory leak
 */
export const cleanQueue = async (grace = 24 * 3600 * 1000) => {
  try {
    const jobs = await reelQueue.clean(grace, 1000, 'completed');
    const failedJobs = await reelQueue.clean(7 * 24 * 3600 * 1000, 1000, 'failed');
    
    logger.info(`ðŸ§¹ Cleaned ${jobs.length} completed and ${failedJobs.length} failed jobs`);
    return { completed: jobs.length, failed: failedJobs.length };
  } catch (error) {
    logger.error('Failed to clean queue:', error);
    return null;
  }
};

// ==================== GRACEFUL SHUTDOWN ====================
const gracefulShutdown = async () => {
  logger.info('ðŸ›‘ Shutting down Queue system...');
  
  try {
    await reelQueue.close();
    
    await connectionQueue.quit();
    await connectionWorker.quit();
    await connectionEvents.quit();
    
    logger.info('âœ… Queue system closed gracefully');
  } catch (error) {
    logger.error('Error during queue shutdown:', error);
  }
};

process.on('SIGTERM', gracefulShutdown);
process.on('SIGINT', gracefulShutdown);

export default connectionQueue;